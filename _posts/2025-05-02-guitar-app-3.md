---
layout: post
title: "기타 연주 앱 프로젝트 - 기타 사운드 분류 로직 구현하기 (1)"
date: 2025-08-21
tags: [Flutter, Audio Engineering, iOS, Android, AI, FFT]
read_time: 15
subtitle: "FFT 스펙트럼의 형태를 파악하여, 기타와 비기타 사운드를 분류해봅니다."
sgst_post: ["2025-04-30-guitar-app-2.md"]
---

## 개요
오랜만에 포스팅을 작성합니다.

그동안 프로젝트의 방향성에도 변동이 잦았고, 필요 기능을 구현하기 위한 기술을 선택하는 과정에서 많은 시행착오를 겪었습니다.

이번 포스팅에는, **기타 사운드 분류 로직**을 구현하면서 겪은 시행착오와, 최종적으로 채택한 YAMNet 모델과 TFLite를 활용한 방법을 공유하고자 합니다.

## 기타 사운드 분류 로직이 뭔가요?
저는 의도적으로 제가 만들고 있는 통칭 **기타 연주 앱 프로젝트**가 정확히 무엇인지 말씀드리지 않고 있습니다.

실제 출시가 목적이기도 하고, 대상 타겟이나 필요 기능 우선순위가 꾸준히 바뀌어 명확히 말씀드리기 어려운 것도 사실입니다.

하지만, 프로젝트 구상 초기부터 필수로 개발해야 하는 기능이 있었는데, 그중 하나가 바로 입력되는 오디오 데이터가 기타 사운드인지, 아니면 이외의 소음인지 구분하는 필터링 기능입니다.

사실, 이 기능을 간단하게 해결하는 방법이 있긴 합니다.

<figure>
  <img src="/assets/images/post-250502-01.jpg.webp" alt="" class="screenshot">
  <figcaption>기타와 케이블로 연결된 오디오 인터페이스. Source - Sweetwater</figcaption>
</figure>

애초에 기능을 구현할 필요가 없는 환경을 대상으로 앱을 개발하면 됩니다.

오디오 인터페이스를 사용하면, 케이블로 연결된 순수한 기타 사운드를 컴퓨터로 입력받을수 있어, 마이크 캡쳐 처럼 소음의 걱정이 아예 없습니다.

하지만, 제 앱의 타겟은 오디오 인터페이스 사용자들이 아니며, 최대한 다양한 타겟을 흡수하기 위해 마이크 캡쳐를 사용하는 것이 좋다고 판단했습니다. 애초에 모바일 앱이라 오디오 인터페이스를 사용할 수 없는 환경인 것도 있습니다. (가능하긴 하나 어려움)

.. 그렇게 삽질이 시작되었습니다.

## V1. 룰 베이스 기반 로직
첫번째 시도로, AI를 활용하지 않고, 룰-베이스 기반으로 기타 사운드 분류 로직을 개발했었습니다.

이 분류 로직은, [이전 포스트](https://dc143cdev.github.io/guitar-app-2/)에서 소개한 오디오 데이터 스트림을 적극 활용하여, 소리의 구성 성분을 파악해 이 소리가 기타 소리인지, 아니면 그 외의 소리인지 구분하는 방식입니다.

후술할 실제 구현은 복잡하지만, 핵심 아이디어는 간단했습니다.

1. **진폭(볼륨)**이 너무 낮거나 높진 않은가?
2. **주파수**의 높낮이가 기타 가역 범위 내에 있는가? (일반적 형태의 6현 기타의 경우, 통상적으로 80Hz~1300Hz 사이)
3. **FFT 스펙트럼**에서 기타만의 특징(배음 구조)이 보이는가?

이 세 가지 조건을 모두 만족하면 기타 소리로 판단하고, 그렇지 않으면 그 외의 소리로 판단하는 방식입니다.

실제로 이 조건 중 앞의 1, 2번 조건은 [과거 AudioKit 라이브러리를 소개하는 포스트](https://dc143cdev.github.io/audiokit/)에서 이미 구현된 거나 다름 없었습니다.

단순히 특정 필터를 거치기만 하면되기 때문입니다.

하지만, FFT 스펙트럼에서 기타만의 특징을 찾는 것은 상당히 어려웠습니다.

### 배음이란?
배음(Overtone)은 물체가 진동할 때 발생하는 기본적인 음인 **기음(Fundamental Tone)**의 정수배 주파수를 가진 음들을 말합니다. 모든 소리는 단 하나의 주파수로 이루어진 것이 아니라, 기음과 함께 여러 개의 배음이 복합적으로 섞여 있습니다. 

이 배음들이 어떻게 구성되고 얼마나 강한지에 따라 소리의 질감, 즉 **음색(Timbre)**이 결정됩니다.

특히, 기타나 바이올린과 같은 현악기는 줄의 양 끝이 고정되어 있어 진동이 반사되며 특유의 파동을 형성합니다. 이때 줄 전체가 한 번에 진동하는 것이 기음이며, 동시에 1/2, 1/3, 1/4 등 여러 부분으로 나뉘어 진동하는 것이 배음입니다.

### FFT 스펙트럼으로 배음 파악하기
배음은 소리의 질감, 즉 **음색(Timbre)**을 결정하는 중요한 요소이지만, 추상적인 개념입니다.

이 추상적인 배음 구조를 시각적으로 확인하기 위해 **FFT(고속 푸리에 변환)**가 필요합니다. 

FFT는 시간 영역의 복합적인 소리 신호를 주파수 영역으로 변환하여, 소리를 구성하는 **기음과 모든 배음의 주파수 및 상대적인 크기(진폭)**를 정량적으로 분석해 줍니다.

[영상: 기타와 보컬 사운드 FFT 스펙트럼 비교]

위 영상은 제가 만든 테스트 앱에서 기타 사운드와 다른 소리의 FFT 스펙트럼 변화를 비교한 것입니다. 보시다시피 기타를 연주할 때는 규칙적인 봉우리들이 나타나지만, 보컬 사운드나 일반 소음은 그렇지 않습니다.

이 규칙적인 봉우리들이 바로 현악기 특유의 배음 구조가 시각적으로 표현된 모습이며, FFT 스펙트럼을 통해 기타 소리를 분류할 수 있다는 아이디어가 여기서 시작되었습니다.

### 실제 구현

이제 본격적인 룰 베이스 버전의 기타 사운드 분류기의 구현을 소개드립니다.

##### 앱 전체에서의 파이프라인

<figure>
  <img src="/assets/images/post-250502-02.png" alt="" class="screenshot">
  <figcaption>앱 전체에서의 파이프라인</figcaption>
</figure>

위 다이어그램은 앱 전체에서의 파이프라인에서 기타 사운드 분류 로직이 위치한 부분을 나타낸 것입니다.

붉은색으로 하이라이트 처리된 **fft_analysis.dart** 와 **fft_noise_filter.dart**가 이번 포스팅의 주인공으로,

[이전 포스트에서 소개드린 중앙화된 오디오 스트림](https://dc143cdev.github.io/guitar-app-2/)으로부터 이어집니다.

각 네이티브 라이브러리 영역 -> 플러터 코어로 이어진 중앙화 스트림에서, 정규화된 세가지 오디오 데이터(FFT, Pitch, 진폭)를 받아, 로직이 입력된 데이터가 기타 사운드인지, 아닌지 판별합니다.

##### 분류기 로직의 내부 파이프라인

<figure>
  <img src="/assets/images/post-250502-03.png" alt="" class="screenshot">
  <figcaption>분류기 로직의 내부 파이프라인</figcaption>
</figure>

위 다이어그램은 전체 구조도에서 표현한 fft_analysis.dart 와 fft_noise_filter.dart 의 내부 파이프라인을 나타낸 것입니다.

실제 소스 코드와 함께, 다이어그램 내부의 각 파이프라인이 어떻게 흘러가는지 설명드립니다.

##### 입력과 결과
~~~dart
class FftSoundClassificationResult {
  /// 0.0~1.0 신뢰도. 후단에서 임계값과 비교해 isGuitarSound를 결정합니다.
  final double confidence;

  /// 최종 이진 판정(임계값 이상이면 true)
  final bool isGuitarSound;

  /// 분석 시각(로그·동기화 용도)
  final DateTime timestamp;

  /// 부가 메타(피크/배음/분포/최대진폭 등 디버깅에 유용)
  final Map<String, dynamic>? metadata;

  const FftSoundClassificationResult({
    required this.confidence,
    required this.isGuitarSound,
    required this.timestamp,
    this.metadata,
  });
}
~~~

