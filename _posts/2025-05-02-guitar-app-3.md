---
layout: post
title: "기타 연주 앱 프로젝트 - 기타 사운드 분류 로직 구현하기"
date: 2025-08-22
tags: [Flutter, Audio Engineering, iOS, Android, AI, TFLite, YAMNet]
read_time: 15
subtitle: "YAMNet 모델과 TFLite를 활용하여 기타 사운드 분류 로직을 구현해봅니다"
sgst_post: ["2025-04-30-guitar-app-2.md"]
---

## 개요
오랜만에 포스팅을 작성합니다.

그동안 프로젝트의 방향성에도 변동이 잦았고, 필요 기능을 구현하기 위한 기술을 선택하는 과정에서 많은 시행착오를 겪었습니다.

이번 포스팅에는, **기타 사운드 분류 로직**을 구현하면서 겪은 시행착오와, 최종적으로 채택한 YAMNet 모델과 TFLite를 활용한 방법을 공유하고자 합니다.

## 기타 사운드 분류 로직이 뭔가요?
저는 의도적으로 제가 만들고 있는 통칭 **기타 연주 앱 프로젝트**가 정확히 무엇인지 말씀드리지 않고 있습니다.

실제 출시가 목적이기도 하고, 대상 타겟이나 필요 기능 우선순위가 꾸준히 바뀌어 명확히 말씀드리기 어려운 것도 사실입니다.

하지만, 프로젝트 구상 초기부터 필수로 개발해야 하는 기능이 있었는데, 그중 하나가 바로 입력되는 오디오 데이터가 기타 사운드인지, 아니면 이외의 소음인지 구분하는 필터링 기능입니다.

사실, 이 기능을 간단하게 해결하는 방법이 있긴 합니다.

[이미지]

애초에 기능을 구현할 필요가 없는 환경을 대상으로 앱을 개발하면 됩니다.

오디오 인터페이스를 사용하면, 케이블로 연결된 순수한 기타 사운드를 컴퓨터로 입력받을수 있어, 마이크 캡쳐 처럼 소음의 걱정이 아예 없습니다.

하지만, 제 앱의 타겟은 오디오 인터페이스 사용자들이 아니며, 최대한 다양한 타겟을 흡수하기 위해 마이크 캡쳐를 사용하는 것이 좋다고 판단했습니다. 애초에 모바일 앱이라 오디오 인터페이스를 사용할 수 없는 환경인 것도 있습니다. (가능하긴 하나 어려움)

.. 그렇게 삽질이 시작되었습니다.

## 시행착오
첫번째 시도로, AI를 활용하지 않고, 룰-베이스 기반으로 기타 사운드 분류 로직을 개발했었습니다.

이 분류 로직은, [이전 포스트](https://jongwon-kim.github.io/2025/04/30/guitar-app-2/)에서 소개한 오디오 데이터 스트림을 적극 활용하여, 소리의 구성 성분을 파악해 이 소리가 기타 소리인지, 아니면 그 외의 소리인지 구분하는 방식입니다.

후술할 실제 구현은 복잡하지만, 핵심 아이디어는 간단했습니다.

1. **진폭(볼륨)**이 너무 낮거나 높진 않은가?
2. **주파수**의 높낮이가 기타 가역 범위 내에 있는가? (일반적 형태의 6현 기타의 경우, 통상적으로 80Hz~1300Hz 사이)
3. **FFT 스펙트럼**에서 기타만의 특징(배음 구조)이 보이는가?

이 세 가지 조건을 모두 만족하면 기타 소리로 판단하고, 그렇지 않으면 그 외의 소리로 판단하는 방식입니다.

실제로 이 조건 중 앞의 1, 2번 조건은 [과거 AudioKit 라이브러리를 소개하는 포스트](https://jongwon-kim.github.io/2025/04/30/guitar-app-1/)에서 이미 구현된 거나 다름 없었습니다.

단순히 특정 필터를 거치기만 하면되기 때문입니다.

하지만, FFT 스펙트럼에서 기타만의 특징을 찾는 것은 상당히 어려웠습니다.

### 배음이란?




