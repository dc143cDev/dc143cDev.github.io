---
layout: post
title: "Audio Kit 라이브러리를 이용한 기타 음 식별 앱 만들기"
date: 2024-11-24
tags: [iOS, Swift, Flutter, AudioKit, FFT]
read_time: 20
subtitle: "기타 음성을 오디오 데이터로 변환하고, FFT(고속 퓨리에 변환) 알고리즘을 구현하여 음 식별 기능을 구현해봅니다"
---

## 개요
기타 연주는 제 오랜 취미입니다. 시간이 나면 늘 연습을 하고, 유튜브 알고리즘은 관련 영상으로 도배되어 있으며, 악기를 살 돈도 없지만 한 달에 한 번은 괜히 낙원상가에 가보곤 합니다.

프로그래밍 역시 제 오랜 취미이자 삶의 일부입니다. 직업으로 삼을 정도니까요. 그렇다보니 문득 기타와 프로그래밍, 제 흥미를 양분하는 이 두 가지 분야를 연결하는 프로젝트를 진행해보면 어떨까 하는 아이디어가 떠올랐습니다.

프로젝트는 지금 이 순간에도 진행 중입니다. 앱의 자세한 기획과 아이디어를 전부 공유하기엔 조금 이르지만, 개발을 진행하는 중간 중간 제가 배우고 느낀 점들을 공유하고자 합니다.

이 포스트는 그 첫 번째 단계로, 기타 소리를 오디오 데이터로 변환하고 식별하는 기능을 구현하는 과정을 다룹니다.


## AudioKit 이란?

[AudioKit GitHub 레포지토리](https://github.com/AudioKit/AudioKit)

[AudioKit 쿡북 앱 GitHub 레포지토리](https://github.com/AudioKit/Cookbook)

AudioKit은 Swift로 작성된 오디오 라이브러리입니다. AudioKit은 실시간 신호 처리, 음향 효과 적용, 오디오 분석 등을 지원하여 복잡한 오디오 작업을 손쉽게 구현할 수 있습니다.

제가 기획한 앱 역시 구현을 위해 오디오 데이터를 다루는 과정이 필수적이었습니다.

저는 앱의 성능과 크로스 플랫폼 개발의 이점을 얻기 위해 Swift로 오디오 처리 및 식별 등의 고급 기능을 구현하고, Flutter로 앱의 UI 영역과 기본적인 기능을 연결하는 구조를 계획했습니다. 

이중 애플 네이티브 생태계에서 오디오 처리 기능을 구현하기 위해 AudioKit을 선택하게 되었습니다.


<figure>
  <img src="/assets/images/post-241124-01.png" alt="AudioKit CookBook App 스크린샷" class="screenshot">
  <figcaption>AudioKit CookBook App 스크린샷</figcaption>
</figure>


AudioKit은 오픈소스 라이브러리입니다.

위 스크린샷은 AudioKit 팀에서 제공하는 쿡북 앱으로, 다양한 예제와 샘플 코드를 통해 AudioKit의 기능을 학습할 수 있는 데모 앱입니다.

소스코드 역시 공개되어 있으며, 저는 이 쿡북 앱의 Tuner 모듈을 참고하여 기타 음성을 오디오 데이터로 변환하고 식별하는 기능을 구현하였습니다.


## 결국 음악 이론은 필요해
당장에 제가 구현한 기타 음 식별 기능을 소개해드리고 싶지만, 안타깝게도 그 전에 넘어야 할 지루한 과정이 있습니다.

기타의 음성을 녹음하여 오디오 데이터로 전환하고, 이것을 우리에게 익숙한 음이름 체계로 변환하는 과정은 약간의 음악 이론 지식이 필요합니다.

기타의 소리는 무엇으로 이루어져 있는지, 주파수는 무엇이고, 또 어떻게 변화하는지, 그리고 이것을 식별하여 우리에게 익숙한 음이름으로 변환할 수 있는 방법은 어떤 건지... 등은 알아야 위의 기능을 구현할 수 있으니까요.

### 주파수가 뭔가요?
제가 구현하고자 하는 기타 음 식별 기능에는 주파수 분석이 필요합니다.

주파수. 음악 이론에 관심이 없는 사람들도 들어본 적이 있을 만큼 익숙한 단어지만, 정작 주파수가 정확히 무엇인지 설명할 수 있는 사람은 많지 않습니다. 저조차도 그랬습니다.

<figure>
  <img src="/assets/images/post-241124-02.png" alt="소리와 주파수" class="screenshot">
  <figcaption>소리와 주파수</figcaption>
</figure>


모든 소리는 파동입니다. 파동은 보통 공기와 같은 매질을 통해 전달되며, 이 파동의 **진동수(Hz, 헤르츠; 초당 진동 수)**를 **주파수**라고 합니다.

파동의 진동수는 소리의 **높낮이**를, 파동의 **진폭**은 소리의 **크기(dB, 데시벨)**를 결정합니다. 소리의 높낮이는 주파수에 따라 결정되며, 주파수가 높을수록 음이 높아지고, 주파수가 낮을수록 음이 낮아집니다.

당연하지만 기타의 현을 튕길 때도 파동이 일어납니다. 이 현의 진동수를 통해 주파수를 결정할 수 있습니다.

현의 두께, 장력, 길이 등이 진동수에 영향을 미치며, 프렛을 눌러 현의 길이를 조절함으로써 다양한 음정을 만들어냅니다. 

즉, 기타를 타현할 때의 주파수를 알아내면 음정과 음이름을 알아낼수 있겠죠. 우리가 AudioKit을 사용하여 분석해야 할 것이 바로 이것입니다.


## 물리적 파동에서 디지털 데이터로, 데이터를 음 이름으로
이제 기타 음 식별 기능을 구현을 위해 무엇이 필요한지 알았으니, 만들어볼 차례입니다.

<figure>
  <img src="/assets/images/post-241124-03.png" alt="기타 음 식별 기능 작동 시퀀스" class="screenshot">
  <figcaption>기타 음 식별 기능 작동 시퀀스</figcaption>
</figure>

위 시퀀스 구조도는 우리가 구현할 기타 음 식별 기능의 시퀀스를 간략하게 보여줍니다.

- 앱 사용자가 기타를 연주(현을 튕겨 음파를 발생)한다. 
- 마이크로 음파(아날로그 신호)를 캡쳐한다.
- AudioProcessor 코드에서 아날로그 신호를 PCM 형식의 디지털 신호로 변환한다
- 변환한 디지털 신호를 FFT 알고리즘을 통해 주파수를 분석한다.
- 분석한 주파수를 음 이름에 매핑한다할
- UI에 음 이름을 표시한다. 유저가 이를 시인한다.

PCM, FFT 등.. 익숙하지 않은 용어들이 난무하고 있습니다. 상술한 시퀀스를 구현하는 코드들과 함께 용어의 정의와 구현하는 과정을 차근차근 알아보도록 합시다.

### 마이크로 기타 음파를 캡쳐
기타 음 식별 기능 구현의 첫 번째 과정은 유저가 연주하는 기타의 음파를 캡쳐하는 것입니다.

~~~swift
// 오디오 처리를 위한 필수 프레임워크들
import AudioKit
import AVFoundation
import AudioToolbox

class AudioProcessor {
    // MARK: - Properties
    private let engine = AudioEngine()  // 오디오 엔진
    private var mic: AudioEngine.InputNode  // 마이크 입력
    private var inputGain: Fader  // 입력 게인 조절
    
    // MARK: - 오디오 세션 설정
    private func setupAudioSession() throws {
        do {
            // 오디오 세션 카테고리 설정
            // .playAndRecord: 녹음과 재생 모두 가능
            // options: 블루투스 등 다양한 입력 장치 지원
            try Settings.session.setCategory(.playAndRecord,
                                           options: [.defaultToSpeaker,
                                                   .mixWithOthers,
                                                   .allowBluetooth,
                                                   .allowBluetoothA2DP])
            // 오디오 세션 활성화
            try Settings.session.setActive(true)
        } catch {
            print("오디오 설정 실패:", error)
            print("Error details:", error.localizedDescription)
            fatalError(error.localizedDescription)
        }
    }
}
~~~


음성 캡쳐를 위해서는, 먼저 오디오 세션을 설정해야 합니다. 위 코드는 통상적인 컨트롤러 코드의 initial 부분에 대응한다고 생각하셔도 무방합니다.

AudioKit의 전역적인 설정은 `Settings` 클래스에서 관리합니다.

해당 코드에서 오디오 세션의 카테고리는 어떤 방식을 사용할 것인지(예: 녹음과 재생 모두 가능한 형태. 혹은 녹음만 가능한 형태 등), 옵션은 어떤 추가적인 기능을 사용할 것인지(예: 블루투스 등)를 결정합니다.

~~~swift
class AudioProcessor {
    ///위의 오디오 세션 코드 이후에 작성됩니다.

    // MARK: - 마이크 초기화 및 설정
    private func setupMicrophone() {
        // 마이크 입력 확인
        guard let input = engine.input else {
            fatalError("마이크를 찾을 수 없습니다")
        }
        guard let device = engine.inputDevice else {
            fatalError("입력 장치를 찾을 수 없습니다")
        }
        
        // 마이크 및 오디오 처리 체인 설정
        mic = input
        inputGain = Fader(mic, gain: 3.0)  // 입력 신호 증폭
        tappableNodeA = Fader(inputGain)
        tappableNodeB = Fader(tappableNodeA)
        silence = Fader(tappableNodeB, gain: 0)
        engine.output = silence
        
        // 피치 감지를 위한 탭 설정 (중요!)
        pitchTap = PitchTap(inputGain) { [weak self] pitch, amp in
            DispatchQueue.main.async {
                self?.update(pitch[0], amp[0], at: CACurrentMediaTime())
            }
        }
    }
    
    // MARK: - 오디오 엔진 시작
    func start() {
        do {
            try engine.start()  // 오디오 엔진 시작
            pitchTap.start()    // 피치 감지 시작
        } catch {
            print("AudioEngine 시작 실패:", error)
        }
    }
}
~~~


그 후엔, 마이크와 오디오 엔진을 초기화하고 시작하는 과정이 필요합니다.

이 부분에서는 마이크의 게인(입력 신호 증폭. 저는 3.0으로 설정하였습니다)을 조절하고, 오디오 처리 체인(오디오 엔진의 입력과 출력을 연결하는 역할)을 설정하고, 오디오 신호를 전달받을 탭을 설정합니다.

**탭**은 오디오 신호를 처리하는 데 사용되는 객체로, AudioKit은 목적에 따라 다양한 탭을 제공합니다. 저는 음의 높낮이 감지를 위한 **PitchTap**을 사용했습니다.

PitchTap을 비롯한 탭 객체들은 쉽게 말해 오디오 데이터 스트림을 특수한 목적에 맞게끔 처리하는 리스너라고 이해할 수 있습니다.


- PitchTap: 피치(주파수) 감지 리스너
- FFTTap: 주파수 스펙트럼 분석 리스낮
- AmplitudeTap: 음량 변화 감지

AudioKit에서 제공하는 탭의 목록은 위와 같으며, PitchTap을 활용하여 기타 소리의 높낮이(Pitch)를 감지하는 부분은 우리가 구현할 기능의 핵심입니다. 

탭의 작동 방식과 활용 방법은 이후 문단에서 집중 조명합니다.


### PCM(Pulse Code Modulation) 변조...?

그럼, 이제 PCM 변조를 구현해야 할까요? 사실, PCM 변조는 AVFoundation 프레임워크에서 이미 구현되어 있습니다.

PCM(Pulse Code Modulation)은 오디오 데이터를 디지털 신호로 변환하는 과정입니다.

iOS에서는 AVFoundation 프레임워크가 이 변환을 자동으로 처리해주기 때문에 직접 구현할 필요는 없지만, 오디오 프로그래밍의 기초가 되는 중요한 개념입니다.

PCM은 아날로그 신호를 이산적인 디지털 값으로 변환하는 과정으로, 다음 두 단계로 이루어집니다:

1. 샘플링(Sampling): 연속적인 음파를 일정 간격으로 측정
   - CD 품질의 경우 초당 44,100번 측정 (44.1kHz)
   - 사람이 들을 수 있는 최고 주파수(20kHz)의 2배 이상 샘플링
2. 양자화(Quantization): 측정된 값을 디지털로 변환
   - 16비트 양자화: 65,536개의 서로 다른 값으로 표현
   - 24비트 양자화: 더 정밀한 음질 구현 가능

우리가 구현할 기능은 이미 PCM 변조를 통해 디지털 신호로 이루어진 오디오 데이터를 활용하고 있습니다.

~~~swift
pitchTap = PitchTap(inputGain) { [weak self] pitch, amp in
    DispatchQueue.main.async {
        self?.update(pitch[0], amp[0], at: CACurrentMediaTime())
    }
}
~~~

대표적으로 위의 PitchTap에서 오디오 데이터를 받아 높낮이를 감지하는 과정 역시 변환된 디지털 신호로 처리하고 있으며, 이후 FFT 분석을 통해 주파수를 추출하는데 사용됩니다.


### FFT(Fast Fourier Transform) 분석

<figure>
  <img src="/assets/images/post-241124-04.png" alt="고속 퓨리에 변환" class="screenshot">
  <figcaption>고속 퓨리에 변환</figcaption>
</figure>