---
layout: post
title: "기타 연주 앱 프로젝트 - 실시간 오디오 분석 모듈 구현해보기"
date: 2025-04-30
tags: [Flutter, AudioKit, TarsosDSP, Audio Engineering, iOS, Android]
read_time: 20
subtitle: "IOS의 AudioKit, Java 라이브러리인 TarsosDSP를 활용하여 플랫폼 통합 오디오 데이터 스트림 모듈과 캘리브레이션 기능을 구현해봅니다"
sgst_post: ["2024-11-24-audiokit.md"]
---

## 개요

작년 말에 작성했던 [이전 포스트](https://dc143cdev.github.io/audiokit/)의 내용을 기억하실지 모르겠습니다.

적는 와중에도 변명임을 알지만, 올해 상반기는 시작부터 새로운 프로젝트에 투입되어 제 개인 앱 개발에 많은 시간을 투자하지 못했습니다.

물론 아예 손을 놓고 있던건 아닙니다.

여전히 앱의 기획을 전부 말씀드릴순 없지만(하여 통칭 기타 연주 앱으로 뭉뚱그려 설명드립니다), 앱의 핵심 기능을 위한 기반 작업인 **실시간 오디오 분석** 모듈을 구현하기까지 다양한 시행착오와 고민을 했습니다.

이번 포스트에서는 제가 **실시간 오디오 분석** 모듈을 구현하며 겪은 경험을 공유하고, 최종적으로 완성된 모듈의 구조를 소개하고자 합니다.


## 기능 요구사항

거의 반년에 가까운 시간동안 앱의 기획과 컨셉이 바뀌며, 그에 따른 기능의 요구사항도 많이 변화했습니다.

그럼에도 불구하고 앱의 핵심 기능에 대한 요구사항은 유지되었는데, 

바로 **마이크를 통해 기타 연주의 소리를 수집하고, 이를 분석하여 식별 가능한 데이터로 바꾸는 것**입니다.

상술한 내용에서 계속 말씀드렸던 실시간 오디오 분석 기능이 위의 요구사항을 구현한 것입니다.


<figure>
  <img src="/assets/images/post-250430-01.png" alt="" class="screenshot">
  <figcaption></figcaption>
</figure>


실시간 오디오 분석 기능으로부터 요구되는 데이터는 다음과 같습니다:

- **Pitch(피치, 주파수)**
- **Amplitude(진폭, 소리의 크기)**
- **FFT 스펙트럼(주파수 대역별 진폭)**

위 세 종류의 데이터는 앱의 모든 핵심 기능들에 필수적이며, 앱 전역에서 포괄적으로 활용됩니다.

### 주요 데이터의 활용 예시

#### Pitch

**Pitch-주파수 데이터**는 기타 연주의 음계를 식별하는데 필요합니다. 

유저가 연주중인 소리가 3옥타브 도인지, 4옥타브 레인지 등등.. 이 데이터를 통해 유저가 연주하는 음계를 식별할 수 있습니다. (이전 포스트에서 주로 다루었던 내용이기도 합니다)

#### Amplitude

**Amplitude-진폭 데이터**는 기타 연주의 크기를 식별하는데 필요합니다. 

쉽게 말해 볼륨입니다. 단순히 크기만 식별하는걸 넘어, 아주 낮은 진폭으로 감지되는 주변 생활 소음을 필터링한다거나(추후 후술할 캘리브레이션 기능), 주변 소리와 유저의 연주 소리의 대비를 통해 **유저가 언제 기타를 연주하는지** 에 대한 타이밍 파악도 가능합니다.

#### FFT 스펙트럼

**FFT 스펙트럼(주파수 대역별 진폭)**은 기타 연주의 주요 주파수 대역을 식별하는데 필요합니다. 

FFT를 쉽게 설명드리자면, **소리를 주파수별로 나눠서 어떤 음이 들어있는지 알아내는 기술**로, 이를 통해 단순 피치 검출로는 분석할수 없는 소리의 세부적인 주파수 대역을 파악할 수 있습니다.

예를 들어, FFT 분석을 통해 감지된 주파수 중 C, E, G가 강하게 감지되는 경우, 유저가 C 코드를 타현중이라고 추측할수 있습니다.

추가로, 고음이나 저음의 비율 등을 분석해 현재 연주중인 코드의 분위기도 파악해볼수 있습니다.


## 시행착오

이처럼 Pitch, Amplitude, FFT 세 가지 데이터를 통해 유저의 연주 음정, 연주 강도, 코드 구성과 같은 정보들을 실시간으로 추론할 수 있습니다.

만약 이 데이터들이 매번 정확하고 일관되게 수집된다면, 이후 단계인 코드 감지나 주법 분석 기능도 큰 문제 없이 구축할 수 있었을 것입니다.

하지만 실제로 개발을 진행하면서는 생각보다 많은 시행착오가 있었습니다.

플랫폼별로 오디오 데이터의 해상도나 감지 민감도가 다르고, 동일한 연주 환경에서도 기기마다 결과가 달라지는 등, 다양한 문제들이 모듈 설계를 복잡하게 만들었기 때문입니다.

## 과정이라 적고 삽질이라 읽는다


#### 1차 구상: AudioKit 기반 iOS 중심 설계

처음에는 iOS 네이티브 영역에서 AudioKit을 활용해, 실시간 오디오 스트림 처리와 고급 분석 기능을 모두 구현하고, Flutter는 단순 UI 렌더링에만 사용하는 구조로 설계했습니다. (이전 포스트에서도 다뤘던 구조입니다.)

하지만 실제 구현 과정에서 메소드 채널을 통해 주고받는 오디오 데이터의 양이 지나치게 많아지며 성능 문제가 발생했고, 이후 안드로이드 영역도 유사하게 개발해야 한다는 점에서 양 플랫폼 모두에서 과도한 네이티브 구현 부담이 예상되었습니다. 

결국 개발 효율성이 크게 떨어진다고 판단하여 해당 구조는 폐기했습니다.

#### 2차 구상: iOS 네이티브 앱으로 방향 전환

AudioKit의 분석 정확도와 성능은 뛰어났기 때문에, 아예 Flutter를 제외하고 iOS 네이티브 앱으로 전환하는 방향을 시도했습니다.

하지만 당시에는 지금처럼 **환경 기반 진폭 보정(캘리브레이션)**에 대한 구상이 없었고, 앱 기획 역시 대중적인 모바일 앱보다는 전문가용 툴에 가까운 방향으로 흐르면서 확장성과 시장성 측면에서 한계를 느꼈습니다. 결국 이 역시 폐기하고 방향을 다시 고민하게 되었습니다.

#### 3차 구상: MacOS 전문가용 툴로 시도

이번에는 오디오 인터페이스 장비를 사용하는 기타 연주 환경을 전제로 하여, MacOS 앱으로의 전환을 고려했습니다. 마이크 입력 대신 오디오 인터페이스를 통해 기타 원음을 받아 분석하는 구조로, 전문가용 기능과 실시간 분석을 목표로 했습니다.

그러나 Swift + MacOS 개발은 러닝 커브가 높았고, 동시에 **MacOS라는 플랫폼의 한계(시장성, 배포 범위)**와, 여전히 Android 영역에 대한 미련도 남아 있어, 이 구상도 최종적으로는 보류하게 되었습니다.

#### 최종 구상: 네이티브 분석 + Flutter 통합 구조

지금의 최종 구조는 다음과 같습니다:

저수준 오디오 분석은 각 플랫폼의 네이티브 영역에서만 담당하도록 하여, iOS는 AudioKit, Android는 TarsosDSP를 활용했습니다.

분석된 데이터는 Pitch, Amplitude, FFT의 세 가지를 통일된 형태로 정규화한 후, Flutter와 메소드 채널을 통해 전달됩니다.

이후 Flutter에서는 UI 표현뿐만 아니라, 코드 추론, 주법 감지, 리듬 분석 등 고급 분석 기능까지 담당하도록 설계했습니다.

이 구조는 제가 가장 익숙한 Flutter를 중심으로 개발 속도를 유지하면서, 두 플랫폼의 네이티브 장점을 최대한 활용할 수 있는 방향입니다. 책임이 명확히 분리된 구조 덕분에 유지보수와 기능 확장에도 유리하다는 장점이 있습니다.


## 실시간 오디오 분석

<figure>
  <img src="/assets/images/post-250430-02-04.png" alt="" class="screenshot">
  <figcaption>실시간 오디오 분석 모듈의 구조</figcaption>
</figure>

그렇게 최종 구상에 따라 만들어진 실시간 오디오 분석 모듈의 구조는 위와 같습니다.

직전에 설명드린 대로, 

마이크 인풋 -> 네이티브 영역의 저수준 분석 처리 -> 정규화된 데이터 전달 -> Flutter 영역에서 고급 분석 처리(및 UI 표현) 순으로 이루어집니다.











